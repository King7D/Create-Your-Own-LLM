{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AdamW,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Prepare dataset\n",
    "data_files = {'train': 'Your_Data.txt'}\n",
    "datasets = load_dataset('text', data_files=data_files)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Filter out empty sequences\n",
    "def filter_empty_examples(example):\n",
    "    return len(example['input_ids']) > 0\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.filter(filter_empty_examples)\n",
    "\n",
    "# Create a data collator that will dynamically pad the inputs received\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Debugging statements\n",
    "        print(f\"input_ids shape: {input_ids.shape}\")\n",
    "        print(f\"labels shape: {labels.shape}\")\n",
    "        \n",
    "        # Check for empty sequences\n",
    "        if input_ids.size(1) == 0:\n",
    "            print(\"Empty input_ids encountered. Skipping this batch.\")\n",
    "            continue  # Skip this batch\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f'Average loss: {avg_loss}')\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
